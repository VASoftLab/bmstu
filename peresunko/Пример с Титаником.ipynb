{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>712 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0           0       3    male  22.0      1      0   7.2500        S\n",
       "1           1       1  female  38.0      1      0  71.2833        C\n",
       "2           1       3  female  26.0      0      0   7.9250        S\n",
       "3           1       1  female  35.0      1      0  53.1000        S\n",
       "4           0       3    male  35.0      0      0   8.0500        S\n",
       "..        ...     ...     ...   ...    ...    ...      ...      ...\n",
       "885         0       3  female  39.0      0      5  29.1250        Q\n",
       "886         0       2    male  27.0      0      0  13.0000        S\n",
       "887         1       1  female  19.0      0      0  30.0000        S\n",
       "889         1       1    male  26.0      0      0  30.0000        C\n",
       "890         0       3    male  32.0      0      0   7.7500        Q\n",
       "\n",
       "[712 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]]\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]].values\n",
    "y = df[[\"Survived\"]].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    layers.Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = tf.keras.Sequential()\n",
    "linear_model.add(normalizer)\n",
    "linear_model.add(layers.Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.compile(\n",
    "    optimizer=tf.optimizers.SGD(learning_rate = 0.1),\n",
    "    loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5621 - val_loss: 0.3274\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3770 - val_loss: 0.3366\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3593 - val_loss: 0.3334\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3548 - val_loss: 0.3384\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3538 - val_loss: 0.3194\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3517 - val_loss: 0.3193\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3548 - val_loss: 0.3339\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3555 - val_loss: 0.3297\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3575 - val_loss: 0.3286\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3532 - val_loss: 0.3345\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3541 - val_loss: 0.3287\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3558 - val_loss: 0.3404\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3524 - val_loss: 0.3211\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3498 - val_loss: 0.3226\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3506 - val_loss: 0.3305\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3516 - val_loss: 0.3379\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3574 - val_loss: 0.3263\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3521 - val_loss: 0.3586\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3543 - val_loss: 0.3204\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.3549 - val_loss: 0.3263\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3538 - val_loss: 0.3564\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3564 - val_loss: 0.3304\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3559 - val_loss: 0.3445\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3535 - val_loss: 0.3288\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3523 - val_loss: 0.3202\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3544 - val_loss: 0.3348\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3570 - val_loss: 0.3220\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3540 - val_loss: 0.3351\n",
      "Epoch 29/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3530 - val_loss: 0.3301\n",
      "Epoch 30/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3575 - val_loss: 0.3282\n",
      "Epoch 31/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3559 - val_loss: 0.3271\n",
      "Epoch 32/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3533 - val_loss: 0.3226\n",
      "Epoch 33/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3560 - val_loss: 0.3347\n",
      "Epoch 34/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3521 - val_loss: 0.3378\n",
      "Epoch 35/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3538 - val_loss: 0.3166\n",
      "Epoch 36/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3487 - val_loss: 0.3301\n",
      "Epoch 37/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3552 - val_loss: 0.3211\n",
      "Epoch 38/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3518 - val_loss: 0.3299\n",
      "Epoch 39/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3513 - val_loss: 0.3275\n",
      "Epoch 40/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3537 - val_loss: 0.3453\n",
      "Epoch 41/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3507 - val_loss: 0.3311\n",
      "Epoch 42/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3538 - val_loss: 0.3376\n",
      "Epoch 43/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3550 - val_loss: 0.3340\n",
      "Epoch 44/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3512 - val_loss: 0.3362\n",
      "Epoch 45/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3535 - val_loss: 0.3270\n",
      "Epoch 46/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3493 - val_loss: 0.3248\n",
      "Epoch 47/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3485 - val_loss: 0.3317\n",
      "Epoch 48/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3507 - val_loss: 0.3247\n",
      "Epoch 49/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3520 - val_loss: 0.3359\n",
      "Epoch 50/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3549 - val_loss: 0.3262\n",
      "Epoch 51/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3497 - val_loss: 0.3197\n",
      "Epoch 52/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3523 - val_loss: 0.3382\n",
      "Epoch 53/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3536 - val_loss: 0.3229\n",
      "Epoch 54/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3536 - val_loss: 0.3268\n",
      "Epoch 55/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3530 - val_loss: 0.3250\n",
      "Epoch 56/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3536 - val_loss: 0.3339\n",
      "Epoch 57/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3548 - val_loss: 0.3302\n",
      "Epoch 58/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3570 - val_loss: 0.3569\n",
      "Epoch 59/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3584 - val_loss: 0.3367\n",
      "Epoch 60/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3531 - val_loss: 0.3285\n",
      "Epoch 61/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3542 - val_loss: 0.3330\n",
      "Epoch 62/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3543 - val_loss: 0.3217\n",
      "Epoch 63/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3548 - val_loss: 0.3226\n",
      "Epoch 64/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3543 - val_loss: 0.3360\n",
      "Epoch 65/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3495 - val_loss: 0.3261\n",
      "Epoch 66/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3549 - val_loss: 0.3373\n",
      "Epoch 67/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3544 - val_loss: 0.3313\n",
      "Epoch 68/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3509 - val_loss: 0.3398\n",
      "Epoch 69/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3536 - val_loss: 0.3275\n",
      "Epoch 70/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3530 - val_loss: 0.3213\n",
      "Epoch 71/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3557 - val_loss: 0.3293\n",
      "Epoch 72/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3547 - val_loss: 0.3184\n",
      "Epoch 73/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3528 - val_loss: 0.3192\n",
      "Epoch 74/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3524 - val_loss: 0.3278\n",
      "Epoch 75/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3591 - val_loss: 0.3222\n",
      "Epoch 76/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3500 - val_loss: 0.3247\n",
      "Epoch 77/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3550 - val_loss: 0.3231\n",
      "Epoch 78/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3549 - val_loss: 0.3349\n",
      "Epoch 79/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3487 - val_loss: 0.3288\n",
      "Epoch 80/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3548 - val_loss: 0.3475\n",
      "Epoch 81/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3506 - val_loss: 0.3361\n",
      "Epoch 82/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3512 - val_loss: 0.3247\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3510 - val_loss: 0.3460\n",
      "Epoch 84/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3552 - val_loss: 0.3407\n",
      "Epoch 85/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3522 - val_loss: 0.3191\n",
      "Epoch 86/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3500 - val_loss: 0.3283\n",
      "Epoch 87/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3524 - val_loss: 0.3508\n",
      "Epoch 88/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3494 - val_loss: 0.3380\n",
      "Epoch 89/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3552 - val_loss: 0.3187\n",
      "Epoch 90/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3523 - val_loss: 0.3240\n",
      "Epoch 91/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3526 - val_loss: 0.3321\n",
      "Epoch 92/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3501 - val_loss: 0.3274\n",
      "Epoch 93/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3535 - val_loss: 0.3201\n",
      "Epoch 94/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3526 - val_loss: 0.3203\n",
      "Epoch 95/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3512 - val_loss: 0.3270\n",
      "Epoch 96/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3540 - val_loss: 0.3259\n",
      "Epoch 97/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3520 - val_loss: 0.3440\n",
      "Epoch 98/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3536 - val_loss: 0.3282\n",
      "Epoch 99/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3518 - val_loss: 0.3295\n",
      "Epoch 100/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3497 - val_loss: 0.3193\n",
      "CPU times: total: 5.42 s\n",
      "Wall time: 4.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = linear_model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 830us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7710280373831776"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((linear_model.predict(X_test) > 0.5).astype(int) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5054 - val_loss: 0.4837\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4717 - val_loss: 0.4471\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4422 - val_loss: 0.4164\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4179 - val_loss: 0.3914\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3987 - val_loss: 0.3715\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3832 - val_loss: 0.3555\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3707 - val_loss: 0.3425\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3605 - val_loss: 0.3318\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3518 - val_loss: 0.3232\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3445 - val_loss: 0.3159\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3383 - val_loss: 0.3097\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3327 - val_loss: 0.3043\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3280 - val_loss: 0.2998\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3238 - val_loss: 0.2959\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3201 - val_loss: 0.2926\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3166 - val_loss: 0.2898\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3137 - val_loss: 0.2875\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3114 - val_loss: 0.2854\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3090 - val_loss: 0.2838\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3071 - val_loss: 0.2824\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3052 - val_loss: 0.2810\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3035 - val_loss: 0.2798\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3020 - val_loss: 0.2788\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3007 - val_loss: 0.2778\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2994 - val_loss: 0.2770\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2983 - val_loss: 0.2764\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2972 - val_loss: 0.2758\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2962 - val_loss: 0.2751\n",
      "Epoch 29/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2953 - val_loss: 0.2746\n",
      "Epoch 30/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2946 - val_loss: 0.2742\n",
      "Epoch 31/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2936 - val_loss: 0.2737\n",
      "Epoch 32/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2930 - val_loss: 0.2733\n",
      "Epoch 33/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2923 - val_loss: 0.2728\n",
      "Epoch 34/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2918 - val_loss: 0.2725\n",
      "Epoch 35/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2911 - val_loss: 0.2721\n",
      "Epoch 36/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2906 - val_loss: 0.2717\n",
      "Epoch 37/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2899 - val_loss: 0.2714\n",
      "Epoch 38/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2893 - val_loss: 0.2711\n",
      "Epoch 39/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2888 - val_loss: 0.2708\n",
      "Epoch 40/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2883 - val_loss: 0.2706\n",
      "Epoch 41/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2880 - val_loss: 0.2703\n",
      "Epoch 42/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2875 - val_loss: 0.2700\n",
      "Epoch 43/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2871 - val_loss: 0.2696\n",
      "Epoch 44/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2866 - val_loss: 0.2693\n",
      "Epoch 45/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2863 - val_loss: 0.2689\n",
      "Epoch 46/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2858 - val_loss: 0.2688\n",
      "Epoch 47/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2854 - val_loss: 0.2685\n",
      "Epoch 48/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2850 - val_loss: 0.2683\n",
      "Epoch 49/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2847 - val_loss: 0.2682\n",
      "Epoch 50/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2843 - val_loss: 0.2680\n",
      "Epoch 51/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2839 - val_loss: 0.2679\n",
      "Epoch 52/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2837 - val_loss: 0.2678\n",
      "Epoch 53/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2833 - val_loss: 0.2674\n",
      "Epoch 54/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2831 - val_loss: 0.2673\n",
      "Epoch 55/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2827 - val_loss: 0.2672\n",
      "Epoch 56/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2824 - val_loss: 0.2672\n",
      "Epoch 57/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2821 - val_loss: 0.2669\n",
      "Epoch 58/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2818 - val_loss: 0.2667\n",
      "Epoch 59/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2814 - val_loss: 0.2665\n",
      "Epoch 60/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2812 - val_loss: 0.2663\n",
      "Epoch 61/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2810 - val_loss: 0.2662\n",
      "Epoch 62/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2807 - val_loss: 0.2661\n",
      "Epoch 63/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2804 - val_loss: 0.2660\n",
      "Epoch 64/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2801 - val_loss: 0.2660\n",
      "Epoch 65/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2798 - val_loss: 0.2659\n",
      "Epoch 66/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2796 - val_loss: 0.2658\n",
      "Epoch 67/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2794 - val_loss: 0.2658\n",
      "Epoch 68/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2791 - val_loss: 0.2657\n",
      "Epoch 69/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2788 - val_loss: 0.2656\n",
      "Epoch 70/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2786 - val_loss: 0.2655\n",
      "Epoch 71/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2784 - val_loss: 0.2655\n",
      "Epoch 72/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2783 - val_loss: 0.2655\n",
      "Epoch 73/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2780 - val_loss: 0.2655\n",
      "Epoch 74/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2778 - val_loss: 0.2655\n",
      "Epoch 75/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2776 - val_loss: 0.2655\n",
      "Epoch 76/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2774 - val_loss: 0.2655\n",
      "Epoch 77/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.2771 - val_loss: 0.2654\n",
      "Epoch 78/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.2769 - val_loss: 0.2653\n",
      "Epoch 79/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.2767 - val_loss: 0.2654\n",
      "Epoch 80/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.2765 - val_loss: 0.2653\n",
      "Epoch 81/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2765 - val_loss: 0.2652\n",
      "Epoch 82/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2761 - val_loss: 0.2650\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2759 - val_loss: 0.2651\n",
      "Epoch 84/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2758 - val_loss: 0.2651\n",
      "Epoch 85/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2757 - val_loss: 0.2651\n",
      "Epoch 86/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2755 - val_loss: 0.2653\n",
      "Epoch 87/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2752 - val_loss: 0.2652\n",
      "Epoch 88/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2750 - val_loss: 0.2652\n",
      "Epoch 89/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2748 - val_loss: 0.2653\n",
      "Epoch 90/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2746 - val_loss: 0.2653\n",
      "Epoch 91/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2745 - val_loss: 0.2653\n",
      "Epoch 92/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2744 - val_loss: 0.2653\n",
      "Epoch 93/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2741 - val_loss: 0.2653\n",
      "Epoch 94/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2740 - val_loss: 0.2656\n",
      "Epoch 95/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2738 - val_loss: 0.2655\n",
      "Epoch 96/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2735 - val_loss: 0.2655\n",
      "Epoch 97/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2734 - val_loss: 0.2656\n",
      "Epoch 98/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2733 - val_loss: 0.2657\n",
      "Epoch 99/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2731 - val_loss: 0.2658\n",
      "Epoch 100/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2729 - val_loss: 0.2656\n",
      "7/7 [==============================] - 0s 835us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7757009345794392"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(normalizer)\n",
    "model.add(layers.Dense(units=50, activation=\"relu\"))\n",
    "model.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.SGD(learning_rate=0.1),\n",
    "    loss='mean_absolute_error')\n",
    "\n",
    "history = model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    validation_split = 0.2)\n",
    "np.mean((model.predict(X_test) > 0.5).astype(int) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x27068948220>,\n",
       " <matplotlib.lines.Line2D at 0x2706892afd0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_t = pd.DataFrame(history.history)\n",
    "plt.plot(df_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.fit_transform(df['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sex\"] = lb.fit_transform(df['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>712 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass  Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0           0       3    1  22.0      1      0   7.2500        S\n",
       "1           1       1    0  38.0      1      0  71.2833        C\n",
       "2           1       3    0  26.0      0      0   7.9250        S\n",
       "3           1       1    0  35.0      1      0  53.1000        S\n",
       "4           0       3    1  35.0      0      0   8.0500        S\n",
       "..        ...     ...  ...   ...    ...    ...      ...      ...\n",
       "885         0       3    0  39.0      0      5  29.1250        Q\n",
       "886         0       2    1  27.0      0      0  13.0000        S\n",
       "887         1       1    0  19.0      0      0  30.0000        S\n",
       "889         1       1    1  26.0      0      0  30.0000        C\n",
       "890         0       3    1  32.0      0      0   7.7500        Q\n",
       "\n",
       "[712 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"Pclass\", \"Age\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\"]].values\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно построить заново другую ИНС"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VA\\AppData\\Local\\Temp\\ipykernel_11972\\2725846524.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf.fit(X_train, y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.325, 0.75 , 0.255, 0.15 , 0.7  , 0.62 , 0.76 , 0.94 , 0.02 ,\n",
       "       0.59 , 0.85 , 0.03 , 0.97 , 0.854, 1.   , 0.11 , 0.804, 0.19 ,\n",
       "       0.   , 0.42 , 0.36 , 0.44 , 0.58 , 0.87 , 0.03 , 0.75 , 0.46 ,\n",
       "       0.87 , 0.82 , 0.17 , 0.99 , 0.213, 0.203, 0.61 , 0.337, 0.32 ,\n",
       "       0.01 , 0.76 , 0.71 , 0.99 , 0.14 , 0.01 , 0.25 , 0.9  , 0.31 ,\n",
       "       0.19 , 0.83 , 1.   , 0.08 , 0.15 , 0.253, 0.381, 0.39 , 0.21 ,\n",
       "       0.62 , 0.839, 0.16 , 0.03 , 0.   , 0.443, 0.508, 0.72 , 0.   ,\n",
       "       0.275, 0.26 , 0.69 , 0.04 , 0.39 , 0.2  , 0.04 , 0.07 , 0.08 ,\n",
       "       0.04 , 0.43 , 0.   , 0.745, 0.   , 0.661, 0.27 , 0.88 , 0.03 ,\n",
       "       0.44 , 0.22 , 0.95 , 0.19 , 0.75 , 0.44 , 0.95 , 0.453, 0.81 ,\n",
       "       0.33 , 0.164, 0.507, 0.84 , 0.252, 0.326, 0.   , 0.03 , 0.642,\n",
       "       0.56 , 0.01 , 0.097, 0.87 , 0.03 , 0.37 , 0.15 , 0.61 , 0.32 ,\n",
       "       0.076, 0.91 , 0.44 , 0.14 , 0.475, 0.73 , 0.839, 0.89 , 0.94 ,\n",
       "       0.04 , 0.023, 0.06 , 0.14 , 0.324, 0.382, 0.514, 0.22 , 0.03 ,\n",
       "       0.45 , 0.03 , 0.13 , 0.132, 0.21 , 0.56 , 0.09 , 0.85 , 0.6  ,\n",
       "       0.52 , 0.4  , 0.58 , 0.145, 0.77 , 0.06 , 0.013, 0.69 , 0.16 ,\n",
       "       0.66 , 0.09 , 0.06 , 0.48 , 0.15 , 0.17 , 0.9  , 0.24 , 0.33 ,\n",
       "       0.018, 0.51 , 0.55 , 0.68 , 0.883, 0.25 , 0.267, 0.55 , 0.03 ,\n",
       "       0.06 , 0.22 , 0.02 , 0.32 , 0.451, 0.451, 0.167, 0.97 , 0.534,\n",
       "       0.77 , 0.25 , 0.84 , 0.04 , 0.27 , 0.04 , 0.45 , 0.56 , 0.2  ,\n",
       "       0.14 , 0.455, 0.   , 0.87 , 0.1  , 0.03 , 0.14 , 0.57 , 0.63 ,\n",
       "       0.02 , 0.54 , 0.376, 0.92 , 0.21 , 0.97 , 0.772, 0.902, 0.27 ,\n",
       "       0.766, 0.105, 0.339, 0.92 , 0.33 , 0.86 , 0.23 , 1.   , 0.529,\n",
       "       0.19 , 0.282, 0.19 , 0.05 , 0.7  , 0.87 , 0.47 ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7336448598130841"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((rf.predict(X_test) > 0.5).astype(int) == y_test.reshape(y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(y.shape[0])\n",
    "y_train = y_train.reshape(y_train.shape[0])\n",
    "y_test = y_test.reshape(y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV] criterion=squared_error, max_depth=4, n_estimators=50 ...........\n",
      "[CV]  criterion=squared_error, max_depth=4, n_estimators=50, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=4, n_estimators=50 ...........\n",
      "[CV]  criterion=squared_error, max_depth=4, n_estimators=50, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=4, n_estimators=50 ...........\n",
      "[CV]  criterion=squared_error, max_depth=4, n_estimators=50, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=4, n_estimators=50 ...........\n",
      "[CV]  criterion=squared_error, max_depth=4, n_estimators=50, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=4, n_estimators=50 ...........\n",
      "[CV]  criterion=squared_error, max_depth=4, n_estimators=50, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=4, n_estimators=100 ..........\n",
      "[CV]  criterion=squared_error, max_depth=4, n_estimators=100, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=4, n_estimators=100 ..........\n",
      "[CV]  criterion=squared_error, max_depth=4, n_estimators=100, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=4, n_estimators=100 ..........\n",
      "[CV]  criterion=squared_error, max_depth=4, n_estimators=100, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=4, n_estimators=100 ..........\n",
      "[CV]  criterion=squared_error, max_depth=4, n_estimators=100, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=4, n_estimators=100 ..........\n",
      "[CV]  criterion=squared_error, max_depth=4, n_estimators=100, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=4, n_estimators=150 ..........\n",
      "[CV]  criterion=squared_error, max_depth=4, n_estimators=150, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=4, n_estimators=150 ..........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=squared_error, max_depth=4, n_estimators=150, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=4, n_estimators=150 ..........\n",
      "[CV]  criterion=squared_error, max_depth=4, n_estimators=150, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=4, n_estimators=150 ..........\n",
      "[CV]  criterion=squared_error, max_depth=4, n_estimators=150, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=4, n_estimators=150 ..........\n",
      "[CV]  criterion=squared_error, max_depth=4, n_estimators=150, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=5, n_estimators=50 ...........\n",
      "[CV]  criterion=squared_error, max_depth=5, n_estimators=50, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=5, n_estimators=50 ...........\n",
      "[CV]  criterion=squared_error, max_depth=5, n_estimators=50, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=5, n_estimators=50 ...........\n",
      "[CV]  criterion=squared_error, max_depth=5, n_estimators=50, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=5, n_estimators=50 ...........\n",
      "[CV]  criterion=squared_error, max_depth=5, n_estimators=50, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=5, n_estimators=50 ...........\n",
      "[CV]  criterion=squared_error, max_depth=5, n_estimators=50, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=5, n_estimators=100 ..........\n",
      "[CV]  criterion=squared_error, max_depth=5, n_estimators=100, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=5, n_estimators=100 ..........\n",
      "[CV]  criterion=squared_error, max_depth=5, n_estimators=100, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=5, n_estimators=100 ..........\n",
      "[CV]  criterion=squared_error, max_depth=5, n_estimators=100, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=5, n_estimators=100 ..........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=squared_error, max_depth=5, n_estimators=100, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=5, n_estimators=100 ..........\n",
      "[CV]  criterion=squared_error, max_depth=5, n_estimators=100, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=5, n_estimators=150 ..........\n",
      "[CV]  criterion=squared_error, max_depth=5, n_estimators=150, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=5, n_estimators=150 ..........\n",
      "[CV]  criterion=squared_error, max_depth=5, n_estimators=150, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=5, n_estimators=150 ..........\n",
      "[CV]  criterion=squared_error, max_depth=5, n_estimators=150, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=5, n_estimators=150 ..........\n",
      "[CV]  criterion=squared_error, max_depth=5, n_estimators=150, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=5, n_estimators=150 ..........\n",
      "[CV]  criterion=squared_error, max_depth=5, n_estimators=150, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=6, n_estimators=50 ...........\n",
      "[CV]  criterion=squared_error, max_depth=6, n_estimators=50, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=6, n_estimators=50 ...........\n",
      "[CV]  criterion=squared_error, max_depth=6, n_estimators=50, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=6, n_estimators=50 ...........\n",
      "[CV]  criterion=squared_error, max_depth=6, n_estimators=50, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=6, n_estimators=50 ...........\n",
      "[CV]  criterion=squared_error, max_depth=6, n_estimators=50, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=6, n_estimators=50 ...........\n",
      "[CV]  criterion=squared_error, max_depth=6, n_estimators=50, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=6, n_estimators=100 ..........\n",
      "[CV]  criterion=squared_error, max_depth=6, n_estimators=100, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=6, n_estimators=100 ..........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=squared_error, max_depth=6, n_estimators=100, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=6, n_estimators=100 ..........\n",
      "[CV]  criterion=squared_error, max_depth=6, n_estimators=100, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=6, n_estimators=100 ..........\n",
      "[CV]  criterion=squared_error, max_depth=6, n_estimators=100, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=6, n_estimators=100 ..........\n",
      "[CV]  criterion=squared_error, max_depth=6, n_estimators=100, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=6, n_estimators=150 ..........\n",
      "[CV]  criterion=squared_error, max_depth=6, n_estimators=150, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=6, n_estimators=150 ..........\n",
      "[CV]  criterion=squared_error, max_depth=6, n_estimators=150, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=6, n_estimators=150 ..........\n",
      "[CV]  criterion=squared_error, max_depth=6, n_estimators=150, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=6, n_estimators=150 ..........\n",
      "[CV]  criterion=squared_error, max_depth=6, n_estimators=150, total=   0.0s\n",
      "[CV] criterion=squared_error, max_depth=6, n_estimators=150 ..........\n",
      "[CV]  criterion=squared_error, max_depth=6, n_estimators=150, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VA\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 'squared_error'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'squared_error'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m150\u001b[39m],\n\u001b[0;32m      5\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m : [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m],\n\u001b[0;32m      6\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcriterion\u001b[39m\u001b[38;5;124m'\u001b[39m :[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msquared_error\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m }\n\u001b[0;32m      9\u001b[0m GSCV \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mrf, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mGSCV\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m GSCV\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:739\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    737\u001b[0m refit_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 739\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    740\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    741\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:377\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    367\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    368\u001b[0m                               random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    369\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)]\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_joblib_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1048\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1048\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1049\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 864\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    781\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 782\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:165\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    163\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m, y, indices)\n\u001b[1;32m--> 165\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    167\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:1221\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1185\u001b[0m         X_idx_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \n\u001b[0;32m   1188\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1221\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_idx_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_idx_sorted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:328\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    325\u001b[0m         criterion \u001b[38;5;241m=\u001b[39m CRITERIA_CLF[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_,\n\u001b[0;32m    326\u001b[0m                                                  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_)\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 328\u001b[0m         criterion \u001b[38;5;241m=\u001b[39m \u001b[43mCRITERIA_REG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m]\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_,\n\u001b[0;32m    329\u001b[0m                                                  n_samples)\n\u001b[0;32m    331\u001b[0m SPLITTERS \u001b[38;5;241m=\u001b[39m SPARSE_SPLITTERS \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;28;01melse\u001b[39;00m DENSE_SPLITTERS\n\u001b[0;32m    333\u001b[0m splitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplitter\n",
      "\u001b[1;31mKeyError\u001b[0m: 'squared_error'"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "\n",
    "param_grid = {\n",
    "   'n_estimators': [50, 100, 150],\n",
    "   'max_depth' : [4, 5, 6],\n",
    "   'criterion' :['squared_error']\n",
    "}\n",
    "\n",
    "GSCV = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, verbose=2)\n",
    "GSCV.fit(X_train, y_train)\n",
    "GSCV.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'squared_error'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m rf \u001b[38;5;241m=\u001b[39m GSCV\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m----> 2\u001b[0m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m rf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:377\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    367\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    368\u001b[0m                               random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    369\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)]\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_joblib_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1048\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1048\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1049\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 864\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    781\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 782\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:165\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    163\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m, y, indices)\n\u001b[1;32m--> 165\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    167\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:1221\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1185\u001b[0m         X_idx_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \n\u001b[0;32m   1188\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1221\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_idx_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_idx_sorted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:328\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    325\u001b[0m         criterion \u001b[38;5;241m=\u001b[39m CRITERIA_CLF[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_,\n\u001b[0;32m    326\u001b[0m                                                  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_)\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 328\u001b[0m         criterion \u001b[38;5;241m=\u001b[39m \u001b[43mCRITERIA_REG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m]\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_,\n\u001b[0;32m    329\u001b[0m                                                  n_samples)\n\u001b[0;32m    331\u001b[0m SPLITTERS \u001b[38;5;241m=\u001b[39m SPARSE_SPLITTERS \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;28;01melse\u001b[39;00m DENSE_SPLITTERS\n\u001b[0;32m    333\u001b[0m splitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplitter\n",
      "\u001b[1;31mKeyError\u001b[0m: 'squared_error'"
     ]
    }
   ],
   "source": [
    "rf = GSCV.best_estimator_\n",
    "rf.fit(X_train, y_train)\n",
    "rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39mmean((\u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m==\u001b[39m y_test\u001b[38;5;241m.\u001b[39mreshape(y_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:766\u001b[0m, in \u001b[0;36mForestRegressor.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    764\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    765\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 766\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    769\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:412\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    410\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimators_\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39m_validate_X_predict(X, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "np.mean((rf.predict(X_test) > 0.5).astype(int) == y_test.reshape(y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "param_grid = {\n",
    "   'n_estimators': [50, 100, 150, 300, 500],\n",
    "   'max_depth' : [4, 5, 6]\n",
    "}\n",
    "\n",
    "GSCV = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, verbose=2)\n",
    "GSCV.fit(X_train, y_train)\n",
    "GSCV.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = GSCV.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rf.predict(X_test) == y_test.reshape(y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
